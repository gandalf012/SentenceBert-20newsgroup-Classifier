{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO DO:\n",
    "1 - create a custom loading class that generates training examples (anchor, positive examples, negatives examples) from 20 News groups (SentenceLabelDataset ) https://github.com/UKPLab/sentence- transformers/blob/6fcfdfb30f9dfcc5fb978c97ce02941a7aa6ba63/sentence_transf ormers/datasets/SentenceLabelDataset.py.\n",
    "\n",
    "2 - Build a training pipeline and finetune a \"distilbert-base-nli-mean-token\" model with the custom TripletLoss class (triplet generation strategy is what matters)\n",
    "\n",
    "3 - fine an Approximate Nearest Neighbors library and explain my choose in few words\n",
    "\n",
    "Build a basic classification pipeline:\n",
    "   * vectorization of the training set with finetune sBert model\n",
    "\n",
    "   * index all this vector with the Approximate Nearest Neighbors library (ANN)\n",
    "\n",
    "   * Build a knn classifier where the new text input get the same labed as that closest index from the index\n",
    "   \n",
    "   * Benchmark the pipeline with the test set\n",
    "\n",
    "   * Compare the model with the pretrained sBert\n",
    "\n",
    "5 - Create a simple REST API that serves this prediction via a \"/predict\" route (Given a input text it will predict one of the 20 News labels)\n",
    "\n",
    "6 - Create a dockerfile to wrap the code in a docker container\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GJJl9_9k0cWb"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive/My')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S8xPh95c1XLg"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/content/drive/My Drive/Ubisoft_takehome_challenge_MLE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "54uI3iS9uVX6"
   },
   "outputs": [],
   "source": [
    "!pip install -U sentence-transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uA3mqMnMiBxH"
   },
   "source": [
    "# Fine tuning SentenceTranformer\n",
    "\n",
    "#### We first create a custom loading class that generates training examples (anchor, positive examples, negatives examples) from 20 News groups.\n",
    "\n",
    "#### Given a input example(anchor), a postive example will be an example from the same label as input example. Negative example will be an an example from an other label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "opSCviUkWR-P"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from torch.utils.data import Dataset\n",
    "from typing import List\n",
    "import bisect\n",
    "import torch\n",
    "import logging\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers.readers import InputExample\n",
    "from multiprocessing import Pool, cpu_count\n",
    "import multiprocessing\n",
    "\n",
    "\n",
    "class Fetch20newsLabelDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for training with triplet loss.\n",
    "    This dataset takes a list of sentences grouped by their label and uses this grouping to dynamically select a\n",
    "    positive example from the same group and a negative example from the other sentences for a selected anchor sentence.\n",
    "\n",
    "    This dataset should be used in combination with dataset_reader.LabelSentenceReader\n",
    "\n",
    "    One iteration over this dataset selects every sentence as anchor once.\n",
    "\n",
    "    This also uses smart batching like SentenceDataset.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "                 model: SentenceTransformer, \n",
    "                 provide_positive: bool = True,\n",
    "                 provide_negative: bool = True,\n",
    "                 parallel_tokenization: bool = True,\n",
    "                 max_processes: int = 4,\n",
    "                 chunk_size: int = 5000):\n",
    "        \"\"\"\n",
    "        Converts 20news datasets to a SentenceLabelDataset usable to train the model with\n",
    "        SentenceTransformer.smart_batching_collate as the collate_fn for the DataLoader\n",
    "\n",
    "        Assumes only one sentence per InputExample and labels as integers from 0 to max_num_labels\n",
    "        and should be used in combination with dataset_reader.LabelSentenceReader.\n",
    "\n",
    "        Labels with only one example are ignored.\n",
    "\n",
    "        smart_batching_collate as collate_fn is required because it transforms the tokenized texts to the tensors.\n",
    "\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.groups_right_border = []\n",
    "        self.grouped_inputs = []\n",
    "        self.grouped_labels = []\n",
    "        self.num_labels = 0\n",
    "        self.max_processes = min(max_processes, cpu_count())\n",
    "        self.chunk_size = chunk_size\n",
    "        self.parallel_tokenization = parallel_tokenization\n",
    "\n",
    "        if self.parallel_tokenization:\n",
    "            if multiprocessing.get_start_method() != 'fork':\n",
    "                logging.info(\"Parallel tokenization is only available on Unix systems which allow to fork processes. Fall back to sequential tokenization\")\n",
    "                self.parallel_tokenization = False\n",
    "\n",
    "        self.dataset = self.get_dataset()\n",
    "        self.convert_input_examples(self.dataset[0], model)\n",
    "\n",
    "        self.idxs = np.arange(len(self.grouped_inputs))\n",
    "\n",
    "        self.provide_positive = provide_positive\n",
    "        self.provide_negative = provide_negative\n",
    "\n",
    "    def get_dataset(self, trainset: str=\"train\", testset: str=\"test\", validation_rate: float=0.01):\n",
    "      \"\"\"\n",
    "      Convert 20news dataset in Train, dev, and Test set\n",
    "\n",
    "      Each instance of train_set is an InputExample with all the class attributes\n",
    "      \"\"\"\n",
    "      ret = []\n",
    "      for name in [trainset, testset]:\n",
    "          file = fetch_20newsgroups(subset=name, remove=('headers', 'footers','quotes'), shuffle=True)\n",
    "\n",
    "          examples = []\n",
    "          guid=1\n",
    "          for text, target in zip(file.data, file.target):\n",
    "              guid += 1\n",
    "              examples.append(InputExample(guid=guid, texts=[text], label=target))\n",
    "          ret.append(examples)\n",
    "\n",
    "      train_set, test_set = ret\n",
    "      dev_set = None\n",
    "\n",
    "      if validation_rate > 0:\n",
    "          size = int(len(train_set) * validation_rate)\n",
    "          dev_set = train_set[-size:]\n",
    "          train_set = train_set[:-size]\n",
    "          \n",
    "      return train_set, dev_set, test_set\n",
    "\n",
    "    def convert_input_examples(self, examples: List[InputExample], model: SentenceTransformer):\n",
    "        \"\"\"\n",
    "        Converts input examples to a SentenceLabelDataset.\n",
    "\n",
    "        Assumes only one sentence per InputExample and labels as integers from 0 to max_num_labels\n",
    "        and should be used in combination with dataset_reader.LabelSentenceReader.\n",
    "\n",
    "        Labels with only one example are ignored.\n",
    "\n",
    "        :param examples:\n",
    "            the input examples for the training\n",
    "        :param model\n",
    "            the Sentence Transformer model for the conversion\n",
    "        :param is_pretokenized\n",
    "            If set to true, no tokenization will be applied. It is expected that the input is tokenized via model.tokenize\n",
    "        \"\"\"\n",
    "\n",
    "        inputs = []\n",
    "        labels = []\n",
    "\n",
    "        label_sent_mapping = {}\n",
    "        too_long = 0\n",
    "        label_type = None\n",
    "\n",
    "        logging.info(\"Start tokenization\")\n",
    "        if not self.parallel_tokenization or self.max_processes == 1 or len(examples) <= self.chunk_size:\n",
    "            tokenized_texts = [self.tokenize_example(example) for example in examples]\n",
    "        else:\n",
    "            logging.info(\"Use multi-process tokenization with {} processes\".format(self.max_processes))\n",
    "            self.model.to('cpu')\n",
    "            with Pool(self.max_processes) as p:\n",
    "                tokenized_texts = list(p.imap(self.tokenize_example, examples, chunksize=self.chunk_size))\n",
    "\n",
    "        # Group examples and labels\n",
    "        # Add examples with the same label to the same dict\n",
    "        for ex_index, example in enumerate(tqdm(examples, desc=\"Convert dataset\")):\n",
    "            if label_type is None:\n",
    "                if isinstance(example.label, int):\n",
    "                    label_type = torch.long\n",
    "                elif isinstance(example.label, float):\n",
    "                    label_type = torch.float\n",
    "            tokenized_text = tokenized_texts[ex_index][0]\n",
    "\n",
    "            if hasattr(model, 'max_seq_length') and model.max_seq_length is not None and model.max_seq_length > 0 and len(tokenized_text) > model.max_seq_length:\n",
    "                too_long += 1\n",
    "\n",
    "            if example.label in label_sent_mapping:\n",
    "                label_sent_mapping[example.label].append(ex_index)\n",
    "            else:\n",
    "                label_sent_mapping[example.label] = [ex_index]\n",
    "\n",
    "            inputs.append(tokenized_text)\n",
    "            labels.append(example.label)\n",
    "\n",
    "        # Group sentences, such that sentences with the same label\n",
    "        # are besides each other. Only take labels with at least 2 examples\n",
    "        distinct_labels = list(label_sent_mapping.keys())\n",
    "        for i in range(len(distinct_labels)):\n",
    "            label = distinct_labels[i]\n",
    "            if len(label_sent_mapping[label]) >= 2:\n",
    "                self.grouped_inputs.extend([inputs[j] for j in label_sent_mapping[label]])\n",
    "                self.grouped_labels.extend([labels[j] for j in label_sent_mapping[label]])\n",
    "                self.groups_right_border.append(len(self.grouped_inputs)) #At which position does this label group / bucket end?\n",
    "                self.num_labels += 1\n",
    "\n",
    "        self.grouped_labels = torch.tensor(self.grouped_labels, dtype=label_type)\n",
    "        logging.info(\"Num sentences: %d\" % (len(self.grouped_inputs)))\n",
    "        logging.info(\"Sentences longer than max_seqence_length: {}\".format(too_long))\n",
    "        logging.info(\"Number of labels with >1 examples: {}\".format(len(distinct_labels)))\n",
    "\n",
    "\n",
    "    def tokenize_example(self, example):\n",
    "        if example.texts_tokenized is not None:\n",
    "            return example.texts_tokenized\n",
    "\n",
    "        return [self.model.tokenize(text) for text in example.texts]\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        if not self.provide_positive and not self.provide_negative:\n",
    "            return [self.grouped_inputs[item]], self.grouped_labels[item]\n",
    "\n",
    "        # Anchor element\n",
    "        anchor = self.grouped_inputs[item]\n",
    "\n",
    "        # Check start and end position for this label in our list of grouped sentences\n",
    "        group_idx = bisect.bisect_right(self.groups_right_border, item)\n",
    "        left_border = 0 if group_idx == 0 else self.groups_right_border[group_idx - 1]\n",
    "        right_border = self.groups_right_border[group_idx]\n",
    "\n",
    "        if self.provide_positive:\n",
    "            positive_item_idx = np.random.choice(np.concatenate([self.idxs[left_border:item], self.idxs[item + 1:right_border]]))\n",
    "            positive = self.grouped_inputs[positive_item_idx]\n",
    "        else:\n",
    "            positive = []\n",
    "\n",
    "        if self.provide_negative:\n",
    "            negative_item_idx = np.random.choice(np.concatenate([self.idxs[0:left_border], self.idxs[right_border:]]))\n",
    "            negative = self.grouped_inputs[negative_item_idx]\n",
    "        else:\n",
    "            negative = []\n",
    "\n",
    "        return [anchor, positive, negative], self.grouped_labels[item]\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.grouped_inputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MGC3utKZvfXS"
   },
   "source": [
    "#### We then create a function that generate a triplet from set of input_examples for model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TRGhTKDqfceZ"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "def triplets_from_labeled_dataset(input_examples):\n",
    "    # Creates triplets for a [(label, sentence), (label, sentence)...] dataset\n",
    "    # by using each example as anchor and selecting randomly a\n",
    "    # positive instance with the same label and a negative instance with different \n",
    "    triplets = []\n",
    "    label2sentence = defaultdict(list)\n",
    "    for example in input_examples:\n",
    "        label2sentence[example.label].append(example)\n",
    "    \n",
    "    for example in input_examples:\n",
    "        anchor = example\n",
    "\n",
    "        if len(label2sentence[example.label]) < 2:\n",
    "            continue\n",
    "\n",
    "        positive = None\n",
    "        while positive is None or positive.guid == anchor.guid:\n",
    "            positive = random.choice(label2sentence[example.label])\n",
    "\n",
    "        negative = None\n",
    "        while negative is None or negative.label == anchor.label:\n",
    "           negative = random.choice(input_examples)\n",
    "\n",
    "        triplets.append(InputExample(texts=[anchor.texts[0], positive.texts[0], negative.texts[0]]))\n",
    "\n",
    "    return triplets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tEaZ9b7jwAWP"
   },
   "source": [
    "##### Let's fine tuning a distilbert-base-nli-mean-tokens model with our custom loading class using the TripletLoss loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "MwZlUwaz0ZqX",
    "outputId": "87aa61ac-51dd-4387-c149-c653705978fc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 245M/245M [00:15<00:00, 15.3MB/s]\n",
      "Downloading 20news dataset. This may take a few minutes.\n",
      "INFO:sklearn.datasets._twenty_newsgroups:Downloading 20news dataset. This may take a few minutes.\n",
      "Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n",
      "INFO:sklearn.datasets._twenty_newsgroups:Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n",
      "Convert dataset: 100%|██████████| 11201/11201 [00:00<00:00, 310210.17it/s]\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import LoggingHandler, losses\n",
    "from sentence_transformers.evaluation import TripletEvaluator\n",
    "from torch.utils.data import DataLoader\n",
    "from datetime import datetime\n",
    "\n",
    "import logging\n",
    "import os\n",
    "import urllib.request\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s - %(message)s',\n",
    "                    datefmt='%Y-%m-%d %H:%M:%S',\n",
    "                    level=logging.INFO,\n",
    "                    handlers=[LoggingHandler()])\n",
    "\n",
    "# Continue training distilbert-base-nli-mean-tokens on 20news_groups data\n",
    "model_name = 'distilbert-base-nli-mean-tokens'\n",
    "\n",
    "### Create a torch.DataLoader that passes training batch to our model\n",
    "train_batch_size = 16\n",
    "\n",
    "if not os.path.exists('/content/drive/My Drive/Ubisoft_takehome_challenge_MLE/Output'):\n",
    "    os.makedirs('/content/drive/My Drive/Ubisoft_takehome_challenge_MLE/Output')\n",
    "\n",
    "output_path = (\"/content/drive/My Drive/Ubisoft_takehome_challenge_MLE/Output/fine-TripletLoss-20news\"+model_name+\"-\"+datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\"))\n",
    "num_epochs = 2\n",
    "\n",
    "# Load pretrained model\n",
    "model = SentenceTransformer(model_name)\n",
    "\n",
    "logging.info(\"Read 20 News groups datasets\")\n",
    "train_dataset = Fetch20newsLabelDataset(model=model, \n",
    "                                         provide_positive=True,   # True for tripletloss\n",
    "                                         provide_negative=True)\n",
    "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=train_batch_size)\n",
    "train_loss = losses.TripletLoss(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "r3rNsgZXBiGl",
    "outputId": "7053e5f6-c1a0-42ab-ae85-4fe6563e7113"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6517857142857143"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Evaluating model performance before model fine tuning\n",
    "logging.info(\"Read 20 News dev set\")\n",
    "dev_set = train_dataset.dataset[1]\n",
    "dev_evaluator = TripletEvaluator.from_input_examples(triplets_from_labeled_dataset(dev_set),  name='dev')\n",
    "\n",
    "logging.info(\"Performance before fine-tuning:\")\n",
    "dev_evaluator(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ljenXIJbOb1x"
   },
   "outputs": [],
   "source": [
    "### Model Fune tuning\n",
    "warmup_steps = int(len(train_dataset) * num_epochs / train_batch_size * 0.1)  # 10% of train data\n",
    "\n",
    "model.fit(\n",
    "    train_objectives=[(train_dataloader, train_loss)],\n",
    "    evaluator=dev_evaluator,\n",
    "    epochs=num_epochs,\n",
    "    evaluation_steps=1000,\n",
    "    warmup_steps=warmup_steps,\n",
    "    output_path=output_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "DsvB22-oPXWB",
    "outputId": "b908791a-0d28-4e9e-f16c-eefa65e3b786"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8698884758364313"
      ]
     },
     "execution_count": 16,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Evaluate model performance on test set\n",
    "logging.info(\"Read 20 News test set\")\n",
    "test_set = train_dataset.dataset[2]\n",
    "test_evaluator = TripletEvaluator.from_input_examples(triplets_from_labeled_dataset(test_set), name=\"test\")\n",
    "\n",
    "logging.info(\"Evaluating model on test set (after fine tune)\")\n",
    "output_path = \"/content/drive/My Drive/Ubisoft_takehome_challenge_MLE/Output/fine-TripletLoss-20newsdistilbert-base-nli-mean-tokens-2020-08-18_20-39-24\"\n",
    "model = SentenceTransformer(output_path)\n",
    "test_evaluator(model)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "20News_Classifier.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
