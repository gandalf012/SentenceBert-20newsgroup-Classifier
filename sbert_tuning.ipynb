{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "20News_Classifier.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "TO DO:\n",
        "1 - create a custom loading class that generates training examples (anchor, positive examples, negatives examples) from 20 News groups (SentenceLabelDataset ) https://github.com/UKPLab/sentence- transformers/blob/6fcfdfb30f9dfcc5fb978c97ce02941a7aa6ba63/sentence_transf ormers/datasets/SentenceLabelDataset.py.\n",
        "\n",
        "2 - Build a training pipeline and finetune a \"distilbert-base-nli-mean-token\" model with the custom TripletLoss class (triplet generation strategy is what matters)\n",
        "\n",
        "3 - fine an Approximate Nearest Neighbors library and explain my choose in few words\n",
        "\n",
        "Build a basic classification pipeline:\n",
        "   * vectorization of the training set with finetune sBert model\n",
        "\n",
        "   * index all this vector with the Approximate Nearest Neighbors library (ANN)\n",
        "\n",
        "   * Build a knn classifier where the new text input get the same labed as that closest index from the index\n",
        "   \n",
        "   * Benchmark the pipeline with the test set\n",
        "\n",
        "   * Compare the model with the pretrained sBert\n",
        "\n",
        "5 - Create a simple REST API that serves this prediction via a \"/predict\" route (Given a input text it will predict one of the 20 News labels)\n",
        "\n",
        "6 - Create a dockerfile to wrap the code in a docker container\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GJJl9_9k0cWb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/My')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S8xPh95c1XLg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.chdir('/content/drive/My Drive/Ubisoft_takehome_challenge_MLE')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "54uI3iS9uVX6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -U sentence-transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uA3mqMnMiBxH",
        "colab_type": "text"
      },
      "source": [
        "# Fine tuning SentenceTranformer\n",
        "\n",
        "#### We first create a custom loading class that generates training examples (anchor, positive examples, negatives examples) from 20 News groups.\n",
        "\n",
        "#### Given a input example(anchor), a postive example will be an example from the same label as input example. Negative example will be an an example from an other label\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "opSCviUkWR-P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from torch.utils.data import Dataset\n",
        "from typing import List\n",
        "import bisect\n",
        "import torch\n",
        "import logging\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sentence_transformers.readers import InputExample\n",
        "from multiprocessing import Pool, cpu_count\n",
        "import multiprocessing\n",
        "\n",
        "\n",
        "class Fetch20newsLabelDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Dataset for training with triplet loss.\n",
        "    This dataset takes a list of sentences grouped by their label and uses this grouping to dynamically select a\n",
        "    positive example from the same group and a negative example from the other sentences for a selected anchor sentence.\n",
        "\n",
        "    This dataset should be used in combination with dataset_reader.LabelSentenceReader\n",
        "\n",
        "    One iteration over this dataset selects every sentence as anchor once.\n",
        "\n",
        "    This also uses smart batching like SentenceDataset.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, \n",
        "                 model: SentenceTransformer, \n",
        "                 provide_positive: bool = True,\n",
        "                 provide_negative: bool = True,\n",
        "                 parallel_tokenization: bool = True,\n",
        "                 max_processes: int = 4,\n",
        "                 chunk_size: int = 5000):\n",
        "        \"\"\"\n",
        "        Converts 20news datasets to a SentenceLabelDataset usable to train the model with\n",
        "        SentenceTransformer.smart_batching_collate as the collate_fn for the DataLoader\n",
        "\n",
        "        Assumes only one sentence per InputExample and labels as integers from 0 to max_num_labels\n",
        "        and should be used in combination with dataset_reader.LabelSentenceReader.\n",
        "\n",
        "        Labels with only one example are ignored.\n",
        "\n",
        "        smart_batching_collate as collate_fn is required because it transforms the tokenized texts to the tensors.\n",
        "\n",
        "        \"\"\"\n",
        "        self.model = model\n",
        "        self.groups_right_border = []\n",
        "        self.grouped_inputs = []\n",
        "        self.grouped_labels = []\n",
        "        self.num_labels = 0\n",
        "        self.max_processes = min(max_processes, cpu_count())\n",
        "        self.chunk_size = chunk_size\n",
        "        self.parallel_tokenization = parallel_tokenization\n",
        "\n",
        "        if self.parallel_tokenization:\n",
        "            if multiprocessing.get_start_method() != 'fork':\n",
        "                logging.info(\"Parallel tokenization is only available on Unix systems which allow to fork processes. Fall back to sequential tokenization\")\n",
        "                self.parallel_tokenization = False\n",
        "\n",
        "        self.dataset = self.get_dataset()\n",
        "        self.convert_input_examples(self.dataset[0], model)\n",
        "\n",
        "        self.idxs = np.arange(len(self.grouped_inputs))\n",
        "\n",
        "        self.provide_positive = provide_positive\n",
        "        self.provide_negative = provide_negative\n",
        "\n",
        "    def get_dataset(self, trainset: str=\"train\", testset: str=\"test\", validation_rate: float=0.01):\n",
        "      \"\"\"\n",
        "      Convert 20news dataset in Train, dev, and Test set\n",
        "\n",
        "      Each instance of train_set is an InputExample with all the class attributes\n",
        "      \"\"\"\n",
        "      ret = []\n",
        "      for name in [trainset, testset]:\n",
        "          file = fetch_20newsgroups(subset=name, remove=('headers', 'footers','quotes'), shuffle=False)\n",
        "\n",
        "          examples = []\n",
        "          guid=1\n",
        "          for text, target in zip(file.data, file.target):\n",
        "              guid += 1\n",
        "              examples.append(InputExample(guid=guid, texts=[text], label=target))\n",
        "          ret.append(examples)\n",
        "\n",
        "      train_set, test_set = ret\n",
        "      dev_set = None\n",
        "\n",
        "      if validation_rate > 0:\n",
        "          size = int(len(train_set) * validation_rate)\n",
        "          dev_set = train_set[-size:]\n",
        "          train_set = train_set[:-size]\n",
        "          \n",
        "      return train_set, dev_set, test_set\n",
        "\n",
        "    def convert_input_examples(self, examples: List[InputExample], model: SentenceTransformer):\n",
        "        \"\"\"\n",
        "        Converts input examples to a SentenceLabelDataset.\n",
        "\n",
        "        Assumes only one sentence per InputExample and labels as integers from 0 to max_num_labels\n",
        "        and should be used in combination with dataset_reader.LabelSentenceReader.\n",
        "\n",
        "        Labels with only one example are ignored.\n",
        "\n",
        "        :param examples:\n",
        "            the input examples for the training\n",
        "        :param model\n",
        "            the Sentence Transformer model for the conversion\n",
        "        :param is_pretokenized\n",
        "            If set to true, no tokenization will be applied. It is expected that the input is tokenized via model.tokenize\n",
        "        \"\"\"\n",
        "\n",
        "        inputs = []\n",
        "        labels = []\n",
        "\n",
        "        label_sent_mapping = {}\n",
        "        too_long = 0\n",
        "        label_type = None\n",
        "\n",
        "        logging.info(\"Start tokenization\")\n",
        "        if not self.parallel_tokenization or self.max_processes == 1 or len(examples) <= self.chunk_size:\n",
        "            tokenized_texts = [self.tokenize_example(example) for example in examples]\n",
        "        else:\n",
        "            logging.info(\"Use multi-process tokenization with {} processes\".format(self.max_processes))\n",
        "            self.model.to('cpu')\n",
        "            with Pool(self.max_processes) as p:\n",
        "                tokenized_texts = list(p.imap(self.tokenize_example, examples, chunksize=self.chunk_size))\n",
        "\n",
        "        # Group examples and labels\n",
        "        # Add examples with the same label to the same dict\n",
        "        for ex_index, example in enumerate(tqdm(examples, desc=\"Convert dataset\")):\n",
        "            if label_type is None:\n",
        "                if isinstance(example.label, int):\n",
        "                    label_type = torch.long\n",
        "                elif isinstance(example.label, float):\n",
        "                    label_type = torch.float\n",
        "            tokenized_text = tokenized_texts[ex_index][0]\n",
        "\n",
        "            if hasattr(model, 'max_seq_length') and model.max_seq_length is not None and model.max_seq_length > 0 and len(tokenized_text) > model.max_seq_length:\n",
        "                too_long += 1\n",
        "\n",
        "            if example.label in label_sent_mapping:\n",
        "                label_sent_mapping[example.label].append(ex_index)\n",
        "            else:\n",
        "                label_sent_mapping[example.label] = [ex_index]\n",
        "\n",
        "            inputs.append(tokenized_text)\n",
        "            labels.append(example.label)\n",
        "\n",
        "        # Group sentences, such that sentences with the same label\n",
        "        # are besides each other. Only take labels with at least 2 examples\n",
        "        distinct_labels = list(label_sent_mapping.keys())\n",
        "        for i in range(len(distinct_labels)):\n",
        "            label = distinct_labels[i]\n",
        "            if len(label_sent_mapping[label]) >= 2:\n",
        "                self.grouped_inputs.extend([inputs[j] for j in label_sent_mapping[label]])\n",
        "                self.grouped_labels.extend([labels[j] for j in label_sent_mapping[label]])\n",
        "                self.groups_right_border.append(len(self.grouped_inputs)) #At which position does this label group / bucket end?\n",
        "                self.num_labels += 1\n",
        "\n",
        "        self.grouped_labels = torch.tensor(self.grouped_labels, dtype=label_type)\n",
        "        logging.info(\"Num sentences: %d\" % (len(self.grouped_inputs)))\n",
        "        logging.info(\"Sentences longer than max_seqence_length: {}\".format(too_long))\n",
        "        logging.info(\"Number of labels with >1 examples: {}\".format(len(distinct_labels)))\n",
        "\n",
        "\n",
        "    def tokenize_example(self, example):\n",
        "        if example.texts_tokenized is not None:\n",
        "            return example.texts_tokenized\n",
        "\n",
        "        return [self.model.tokenize(text) for text in example.texts]\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        if not self.provide_positive and not self.provide_negative:\n",
        "            return [self.grouped_inputs[item]], self.grouped_labels[item]\n",
        "\n",
        "        # Anchor element\n",
        "        anchor = self.grouped_inputs[item]\n",
        "\n",
        "        # Check start and end position for this label in our list of grouped sentences\n",
        "        group_idx = bisect.bisect_right(self.groups_right_border, item)\n",
        "        left_border = 0 if group_idx == 0 else self.groups_right_border[group_idx - 1]\n",
        "        right_border = self.groups_right_border[group_idx]\n",
        "\n",
        "        if self.provide_positive:\n",
        "            positive_item_idx = np.random.choice(np.concatenate([self.idxs[left_border:item], self.idxs[item + 1:right_border]]))\n",
        "            positive = self.grouped_inputs[positive_item_idx]\n",
        "        else:\n",
        "            positive = []\n",
        "\n",
        "        if self.provide_negative:\n",
        "            negative_item_idx = np.random.choice(np.concatenate([self.idxs[0:left_border], self.idxs[right_border:]]))\n",
        "            negative = self.grouped_inputs[negative_item_idx]\n",
        "        else:\n",
        "            negative = []\n",
        "\n",
        "        return [anchor, positive, negative], self.grouped_labels[item]\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.grouped_inputs)\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MGC3utKZvfXS",
        "colab_type": "text"
      },
      "source": [
        "#### We then create a function that generate a triplet from set of input_examples for model evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TRGhTKDqfceZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "from collections import defaultdict\n",
        "\n",
        "def triplets_from_labeled_dataset(input_examples):\n",
        "    # Creates triplets for a [(label, sentence), (label, sentence)...] dataset\n",
        "    # by using each example as anchor and selecting randomly a\n",
        "    # positive instance with the same label and a negative instance with different \n",
        "    triplets = []\n",
        "    label2sentence = defaultdict(list)\n",
        "    for example in input_examples:\n",
        "        label2sentence[example.label].append(example)\n",
        "    \n",
        "    for example in input_examples:\n",
        "        anchor = example\n",
        "\n",
        "        if len(label2sentence[example.label]) < 2:\n",
        "            continue\n",
        "\n",
        "        positive = None\n",
        "        while positive is None or positive.guid == anchor.guid:\n",
        "            positive = random.choice(label2sentence[example.label])\n",
        "\n",
        "        negative = None\n",
        "        while negative is None or negative.label == anchor.label:\n",
        "           negative = random.choice(input_examples)\n",
        "\n",
        "        triplets.append(InputExample(texts=[anchor.texts[0], positive.texts[0], negative.texts[0]]))\n",
        "\n",
        "    return triplets"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tEaZ9b7jwAWP",
        "colab_type": "text"
      },
      "source": [
        "##### Let's fine tuning a distilbert-base-nli-mean-tokens model with our custom loading class using the TripletLoss loss."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MwZlUwaz0ZqX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "87aa61ac-51dd-4387-c149-c653705978fc"
      },
      "source": [
        "from sentence_transformers import LoggingHandler, losses\n",
        "from sentence_transformers.evaluation import TripletEvaluator\n",
        "from torch.utils.data import DataLoader\n",
        "from datetime import datetime\n",
        "\n",
        "import logging\n",
        "import os\n",
        "import urllib.request\n",
        "\n",
        "logging.basicConfig(format='%(asctime)s - %(message)s',\n",
        "                    datefmt='%Y-%m-%d %H:%M:%S',\n",
        "                    level=logging.INFO,\n",
        "                    handlers=[LoggingHandler()])\n",
        "\n",
        "# Continue training distilbert-base-nli-mean-tokens on 20news_groups data\n",
        "model_name = 'distilbert-base-nli-mean-tokens'\n",
        "\n",
        "### Create a torch.DataLoader that passes training batch to our model\n",
        "train_batch_size = 16\n",
        "\n",
        "if not os.path.exists('/content/drive/My Drive/Ubisoft_takehome_challenge_MLE/Output'):\n",
        "    os.makedirs('/content/drive/My Drive/Ubisoft_takehome_challenge_MLE/Output')\n",
        "\n",
        "output_path = (\"/content/drive/My Drive/Ubisoft_takehome_challenge_MLE/Output/fine-TripletLoss-20news\"+model_name+\"-\"+datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\"))\n",
        "num_epochs = 2\n",
        "\n",
        "# Load pretrained model\n",
        "model = SentenceTransformer(model_name)\n",
        "\n",
        "logging.info(\"Read 20 News groups datasets\")\n",
        "train_dataset = Fetch20newsLabelDataset(model=model, \n",
        "                                         provide_positive=True,   # True for tripletloss\n",
        "                                         provide_negative=True)\n",
        "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=train_batch_size)\n",
        "train_loss = losses.TripletLoss(model)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 245M/245M [00:15<00:00, 15.3MB/s]\n",
            "Downloading 20news dataset. This may take a few minutes.\n",
            "INFO:sklearn.datasets._twenty_newsgroups:Downloading 20news dataset. This may take a few minutes.\n",
            "Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n",
            "INFO:sklearn.datasets._twenty_newsgroups:Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n",
            "Convert dataset: 100%|██████████| 11201/11201 [00:00<00:00, 310210.17it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r3rNsgZXBiGl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7053e5f6-c1a0-42ab-ae85-4fe6563e7113"
      },
      "source": [
        "### Evaluating model performance before model fine tuning\n",
        "logging.info(\"Read 20 News dev set\")\n",
        "dev_set = train_dataset.dataset[1]\n",
        "dev_evaluator = TripletEvaluator.from_input_examples(triplets_from_labeled_dataset(dev_set),  name='dev')\n",
        "\n",
        "logging.info(\"Performance before fine-tuning:\")\n",
        "dev_evaluator(model)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6517857142857143"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ljenXIJbOb1x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Model Fune tuning\n",
        "warmup_steps = int(len(train_dataset) * num_epochs / train_batch_size * 0.1)  # 10% of train data\n",
        "\n",
        "model.fit(\n",
        "    train_objectives=[(train_dataloader, train_loss)],\n",
        "    evaluator=dev_evaluator,\n",
        "    epochs=num_epochs,\n",
        "    evaluation_steps=1000,\n",
        "    warmup_steps=warmup_steps,\n",
        "    output_path=output_path,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DsvB22-oPXWB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b908791a-0d28-4e9e-f16c-eefa65e3b786"
      },
      "source": [
        "### Evaluate model performance on test set\n",
        "logging.info(\"Read 20 News test set\")\n",
        "test_set = train_dataset.dataset[2]\n",
        "test_evaluator = TripletEvaluator.from_input_examples(triplets_from_labeled_dataset(test_set), name=\"test\")\n",
        "\n",
        "logging.info(\"Evaluating model on test set (after fine tune)\")\n",
        "output_path = \"/content/drive/My Drive/Ubisoft_takehome_challenge_MLE/Output/fine-TripletLoss-20newsdistilbert-base-nli-mean-tokens-2020-08-18_20-39-24\"\n",
        "model = SentenceTransformer(output_path)\n",
        "test_evaluator(model)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8698884758364313"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    }
  ]
}